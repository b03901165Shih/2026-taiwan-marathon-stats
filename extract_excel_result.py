import math
import json
from collections import defaultdict
from datetime import datetime
import pandas as pd


# ========= åŸºæœ¬å·¥å…·å‡½å¼ =========

def time_str_to_seconds(t: str) -> int:
    """
    å°‡ 'HH:MM:SS' è½‰æˆç§’æ•¸ (int)ã€‚
    è‹¥æ ¼å¼ä¸åˆæ³•ï¼Œæ‹‹å‡º ValueErrorã€‚
    """
    h, m, s = map(int, str(t).split(":"))
    return h * 3600 + m * 60 + s


def seconds_to_time_str(sec: int) -> str:
    """
    å°‡ç§’æ•¸è½‰å› 'HH:MM:SS' å­—ä¸²ã€‚
    """
    sec = int(sec)
    h = sec // 3600
    m = (sec % 3600) // 60
    s = sec % 60
    return f"{h:02d}:{m:02d}:{s:02d}"



def build_group_keys(row) -> list[tuple[str, str]]:
    """çµ¦ä¸€åˆ—æˆç¸¾ï¼Œå›å‚³å®ƒæ‡‰è©²è¢«æ­¸åˆ°å“ªäº› (è³½åˆ¥, åˆ†çµ„key)"""
    keys: list[tuple[str, str]] = []
    race_type = row["è³½åˆ¥"]
    group = str(row["åˆ†çµ„"])
    
    # 1) è³½åˆ¥ + ALL
    keys.append((race_type, "ALL"))
    # 2) è³½åˆ¥ + åŸå§‹åˆ†çµ„
    keys.append((race_type, group))
    return keys


# ========= è®€å– Excel & æ•´ç†ç§’æ•¸ =========

def load_and_group_seconds(excel_path: str) -> dict[tuple[str, str], list[int]]:
    """
    å¾ Excel è®€å–è³‡æ–™ï¼Œä¾ (è³½åˆ¥, åˆ†çµ„key) å›å‚³å®Œè³½ç§’æ•¸ listã€‚
    Excel æ¬„ä½ï¼ˆA1~I1ï¼‰ï¼š
        å§“å, èƒŒè™Ÿ, è³½åˆ¥, è³½äº‹é¡å‹, åˆ†çµ„, å®Œè³½æ™‚é–“, ä¾†æºåˆ†çµ„æ¨™ç±¤, å®Œè³½æ™‚é–“_td, ç¸½æ’å
    """
    df = pd.read_excel(excel_path)

    required_cols = ["å§“å", "èƒŒè™Ÿ", "è³½åˆ¥", "è³½äº‹é¡å‹", "åˆ†çµ„", "å®Œè³½æ™‚é–“"]
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"Excel ç¼ºå°‘å¿…è¦æ¬„ä½: {col}")

    # ç¯©é¸æœ‰æ•ˆè³‡æ–™ + è½‰ç§’æ•¸
    def safe_time_to_seconds(t):
        """å®‰å…¨è½‰æ›ï¼šç„¡æ•ˆå€¼å›å‚³ None"""
        if pd.isna(t) or str(t).strip() in ['--', '-', 'DNF', 'DNS', '']:
            return None
        try:
            return time_str_to_seconds(str(t))
        except:
            return None

    # å¦‚æœ‰ seconds æ¬„ä½ï¼Œå¯ä»¥ç›´æ¥ç”¨ï¼›å¦å‰‡å¾å®Œè³½æ™‚é–“è½‰
    if "seconds" in df.columns:
        df["seconds"] = df["seconds"].astype(int)
    else:
        df["seconds"] = df["å®Œè³½æ™‚é–“"].apply(safe_time_to_seconds)

    group_seconds: dict[tuple[str, str], list[int]] = defaultdict(list)

    for _, row in df.iterrows():
        time_val = row["å®Œè³½æ™‚é–“"]
        if pd.isna(time_val):
            continue

        try:
            sec = int(row["seconds"])
        except Exception:
            try:
                sec = time_str_to_seconds(time_val)
            except Exception:
                continue

        # DNF / ç©ºå€¼æ’é™¤å¯ä»¥åœ¨é€™è£¡åŠ æ¢ä»¶
        for race_type, key in build_group_keys(row):
            group_seconds[(race_type, key)].append(sec)

    # æ’åº
    for k in group_seconds:
        group_seconds[k].sort()

    return group_seconds


# ========= å»ºç«‹ 5 åˆ†é˜ histogram =========

def build_histograms(group_seconds: dict[tuple[str, str], list[int]],
                     bin_size_sec: int = 5 * 60) -> dict[str, dict]:
    """
    ç‚ºæ¯å€‹ (è³½åˆ¥, åˆ†çµ„key) åš 5 åˆ†é˜ bin çš„ histogramã€‚
    å›å‚³:
        {
          "HM__ALL": {
            "histogram_5min": [
              {"start_sec":..., "end_sec":..., "start_time":..., "end_time":..., "count":...},
              ...
            ]
          },
          ...
        }
    """
    result: dict[str, dict] = {}

    for (race_type, group_key), arr in group_seconds.items():
        if not arr:
            continue
        print(f'KEYS = {(race_type, group_key)} -> len = {len(arr)}')
        min_s = min(arr)
        max_s = max(arr)

        start_bin = int(min_s // bin_size_sec)
        end_bin = int(math.ceil(max_s / bin_size_sec))

        bins = []
        for b in range(start_bin, end_bin + 1):
            lo = b * bin_size_sec
            hi = (b + 1) * bin_size_sec
            # è¨ˆæ•¸ï¼šlo <= sec < hi
            count = sum(1 for x in arr if lo <= x < hi)
            bins.append({
                "start_sec": lo,
                "end_sec": hi,
                "start_time": seconds_to_time_str(lo),
                "end_time": seconds_to_time_str(hi),
                "count": count,
            })

        key = f"{race_type}__{group_key}"
        if key not in result:
            result[key] = {}
        result[key]["histogram_5min"] = bins

    return result


# ========= å„çµ„ summaryï¼ˆäººæ•¸/æœ€çŸ­/æœ€é•·/å¹³å‡/ä¸­ä½æ•¸ï¼‰ =========

def build_sorted_seconds(group_seconds: dict[tuple[str, str], list[int]]) -> dict[str, dict]:
    """å»ºç«‹ sorted_seconds"""
    result: dict[str, dict] = {}
    for (race_type, group_key), arr in group_seconds.items():
        if not arr:
            continue
        key = f"{race_type}__{group_key}"
        result[key] = {
            "sorted_seconds": [int(x) for x in sorted(arr)]
        }
    return result

# ========= ğŸš€ æ“´å……æ€§æœ€ä½³æ–¹æ¡ˆï¼šMetadata çµæ§‹ =========
def create_metadata(event_config: dict) -> dict:
    """å»ºç«‹æ¨™æº–åŒ– metadata"""
    return {
        "event_id": event_config["id"],
        "event_name": event_config["name"],
        "event_date": event_config.get("date", ""),
        "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "total_participants": event_config.get("total_count", 0),
        "race_types": event_config["race_types"],
        "group_categories": event_config.get("group_categories", ["ALL", "ä¸€èˆ¬", "è¼ªæ¤…", "è¦–éšœ"]),
        "data_structure": {
            "histogram_bin_size": "5min",
            "percentile_precision": "0.1%",
            "time_format": "HH:MM:SS"
        }
    }

def build_data(excel_path: str, event_config: dict) -> tuple[dict, dict, dict]:
    """å»ºç«‹å®Œæ•´è³‡æ–™é›†ï¼šcombined + summary + metadata"""
    print("ğŸ”„ è®€å–ä¸¦æ•´ç†ç§’æ•¸ä¸­...")
    group_seconds = load_and_group_seconds(excel_path)
    
    print("ğŸ“Š è¨ˆç®— histogram...")
    hist_json = build_histograms(group_seconds)
    
    print("âš¡ å»ºç«‹ sorted_seconds...")
    sorted_json = build_sorted_seconds(group_seconds)
    
    # åˆä½µæˆ event å‰ç¶´ key
    combined: dict[str, dict] = {}
    all_keys = set(hist_json.keys()) | set(sorted_json.keys())
    for k in all_keys:
        full_key = f"{event_config['id']}__{k}"
        combined[full_key] = {}
        if k in hist_json:
            combined[full_key].update(hist_json[k])
        if k in sorted_json:
            combined[full_key]["sorted_seconds"] = sorted_json[k]["sorted_seconds"]
    
    metadata = create_metadata(event_config)
    return combined,  metadata

def output_event_js(combined: dict, metadata: dict, js_filename: str):
    """è¼¸å‡ºæ¨™æº–åŒ– .js æª”æ¡ˆï¼ŒåŒ…å«å®Œæ•´ metadata"""
    with open(js_filename, "w", encoding="utf-8") as f:
        f.write("// ================================================\n")
        f.write(f"// {metadata['event_name']} å‰è™•ç†è³‡æ–™\n")
        f.write(f"// ç”Ÿæˆæ™‚é–“ï¼š{metadata['generated_at']}\n")
        f.write(f"// ç¸½äººæ•¸ï¼š{metadata['total_participants']}äºº\n")
        f.write("// åŒ…å«ï¼šhistogram_5min + sorted_seconds\n")
        f.write("// ================================================\n\n")
        
        f.write("window.marathonData = window.marathonData || {};\n\n")
        f.write(f"// {metadata['event_name']} è³‡æ–™\n")
        f.write(f"window.marathonData['{metadata['event_id']}'] = ")
        json.dump({
            "metadata": metadata,
            "binsAndPr": combined,
        }, f, ensure_ascii=False, indent=2)
        f.write(f";\n\n")
        
        # çµ±è¨ˆè³‡è¨Šè¨»è§£
        total_keys = len(combined)
        total_races = len(set(k.split('_')[1].split('__')[0] for k in combined))
        total_people = sum(len(v.get('sorted_seconds', [])) for v in combined.values())
        f.write(f"// ğŸ“Š çµ±è¨ˆï¼š{total_races}è³½åˆ¥ Ã— {total_keys}åˆ†çµ„ = {total_people:,}å®Œè³½è¨˜éŒ„\n")
    
    print(f"âœ… è¼¸å‡ºï¼š{js_filename}")
    print(f"   ğŸ“… {metadata['event_name']}")
    print(f"   ğŸ‘¥ {total_people:,}äºº / {total_races}è³½åˆ¥ / {total_keys}åˆ†çµ„")

# ========= ğŸ¯ ä¸»ç¨‹å¼ï¼šæ”¯æ´å¤šè³½äº‹æ“´å…… =========
def main():
    """æ”¯æ´æœªä¾†ç„¡é™æ“´å……æ–°è³½äº‹ï¼"""
    
    # ğŸŒŸ è³½äº‹é…ç½®è¡¨ï¼ˆæœªä¾†åŠ æ–°è³½äº‹åªè¦åŠ ä¸€åˆ—ï¼ï¼‰
    EVENTS = [
        {
            "id": "2025_tpe",
            "name": "2025å°åŒ—é¦¬æ‹‰æ¾",
            "excel": "2025_å°åŒ—é¦¬æ‹‰æ¾_å®Œæ•´æˆç¸¾.xlsx",
            "date": "2025-12-21",
            "race_types": ["MA", "HM"],
            "total_count": 0  # æœƒè‡ªå‹•è¨ˆç®—
        },
        {
            "id": "2026_chartered_tpe", 
            "name": "2026æ¸£æ‰“å°åŒ—å…¬ç›Šé¦¬æ‹‰æ¾",
            "excel": "2026_æ¸£æ‰“å°åŒ—é¦¬æ‹‰æ¾_å®Œæ•´æˆç¸¾.xlsx",
            "date": "2026-01-18",
            "race_types": ["å…¨ç¨‹é¦¬æ‹‰æ¾(42.195KM)", "åŠç¨‹é¦¬æ‹‰æ¾(21.0975km)", "11KM"],
            "total_count": 0
        }
        # æœªä¾†åŠ æ–°è³½äº‹ï¼š
        # {
        #     "id": "2027_tpe_full", 
        #     "name": "2027å°åŒ—é¦¬æ‹‰æ¾",
        #     "excel": "2027_xxx.xlsx",
        #     "date": "2027-12-19",
        #     "race_types": ["MA", "HM"],
        #     "total_count": 0
        # }
    ]
    
    for event in EVENTS:
        try:
            excel_path = event["excel"]
            combined, metadata = build_data(excel_path, event)
            js_filename = f"{event['id']}_data.js"
            output_event_js(combined, metadata, js_filename)
            print()
        except Exception as e:
            print(f"âŒ {event['name']} è™•ç†å¤±æ•—ï¼š{e}")

if __name__ == "__main__":
    main()